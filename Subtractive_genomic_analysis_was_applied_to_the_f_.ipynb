{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nibaskumar93n-debug/Morphoinformatics/blob/main/Subtractive_genomic_analysis_was_applied_to_the_f_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing the **entire subtractive genomic analysis pipeline** as describedâ€”from protein sequence retrieval through essentiality and metabolic pathway analysis to subcellular localizationâ€”**is generally not feasible to execute *entirely* within a Google Colab notebook using the *exact* external web servers mentioned (UniProt, CD-HIT, BLASTp via NCBI web interface, Geptop, KAAS)**.\n",
        "\n",
        "However, **it is absolutely possible to replicate the *steps* and perform *equivalent analyses* using Python libraries and command-line tools that can be installed or run within the Colab environment**, though this requires significant coding and setup.\n",
        "\n",
        "Here is a guide outlining the feasibility and detailing the steps for a **Colab-adapted implementation**.\n",
        "\n",
        "-----\n",
        "\n",
        "## 1\\. Feasibility of Colab Implementation\n",
        "\n",
        "| Step | Original Tool | Feasibility in Colab | Notes on Colab Implementation |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Protein Retrieval** | UniProt Database | **High** | Use **BioPython** to fetch sequences using accession IDs or use UniProt's API. |\n",
        "| **Paralog Discarding** | CD-HIT Server | **High** | Install and run **CD-HIT** (command-line version) in Colab's terminal, or use a Python wrapper if available, or write a custom clustering script using a library like `scikit-learn` or `MMseqs2`. |\n",
        "| **Non-Homologous Identification** | BLASTp (NCBI Web) | **Medium/High** | Use **standalone BLAST+** (easily installed in Colab) and the **BioPython** `NcbiWWW` or `NcbiDblocal` modules. Requires downloading a human proteome database. **This is the most computationally intensive step.** |\n",
        "| **Essentiality Assessment** | Geptop Server | **Low** | Geptop is a proprietary web server. **Cannot be run directly.** You'd need to find a similar **essential gene prediction tool** (e.g., using machine learning models or comparative genomics data) or use a dataset of known essential genes if available. This step is the **hardest to replicate precisely.** |\n",
        "| **Metabolic Pathway Analysis** | KAAS Server | **Low** | KAAS is a specialized web server. **Cannot be run directly.** You would use **BioPython** and the **KEGG REST API** (or similar tools like **GhostKOALA** if they offer an API/standalone version) to assign KOs and map to pathways. This requires careful parsing of results. |\n",
        "| **Subcellular Localization** | (Tool not specified) | **Medium** | Use publicly available **standalone localization prediction tools** like **PSORTb** or **DeepTMHMM** (if available for install) or use an **API** from a service like **DeepLoc** (if one exists). |\n",
        "\n",
        "-----\n",
        "\n",
        "## 2\\. Step-by-Step Guide for Colab-Adapted Subtractive Genomic Analysis\n",
        "\n",
        "### A. Setup and Dependencies\n",
        "\n",
        "The first cell in your Colab notebook will be for installation."
      ],
      "metadata": {
        "id": "SzTW8WuT4uvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q biopython pandas requests\n",
        "!mkdir -p /content/{proteome,non_paralogous,blast_results}\n",
        "import requests, os, pandas as pd\n",
        "from Bio import SeqIO"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.3/3.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/3.2 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "jvZpSIyG4uvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe03b21-c35d-41e6-dca9-880ab6c77c89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Protein Sequence Retrieval (UniProt)\n",
        "\n",
        "Use **BioPython** to fetch the sequences for your four organisms."
      ],
      "metadata": {
        "id": "YHtGK7RP4uv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- STEP 1. Upload your proteome manually from your computer\n",
        "# Go to the left panel â†’ Files tab â†’ Upload your FASTA file manually\n",
        "# Example filename: Bifidobacterium_animalis.fasta\n",
        "# Then move it into the right folder:\n",
        "\n",
        "uploaded_proteome = \"/content/proteome/uniprotkb_proteome_UP000037239_2025_10_29.fasta\"\n",
        "species_name = \"Bifidobacterium_animalis\"\n",
        "\n",
        "if os.path.exists(uploaded_proteome):\n",
        "    os.rename(uploaded_proteome, f\"/content/proteome/{species_name}.fasta\")\n",
        "    proteome_path = f\"/content/proteome/{species_name}.fasta\"\n",
        "    print(f\"âœ… Proteome uploaded: {proteome_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"âŒ Please upload your FASTA file manually in Colab first!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Proteome uploaded: /content/proteome/Bifidobacterium_animalis.fasta\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofGmuMOx4uv1",
        "outputId": "fdc5944c-0f7e-4630-9a3b-5da2518586e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Paralog Discarding (CD-HIT)\n",
        "\n",
        "Run the **CD-HIT** command-line tool within Colab using the `!` prefix."
      ],
      "metadata": {
        "id": "PxAieQhp4uv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2. Remove paralogous sequences using CD-HIT (60% identity)\n",
        "!apt-get install -y cd-hit\n",
        "\n",
        "non_paralog_path = f\"/content/non_paralogous/{species_name}_nonparalog.fasta\"\n",
        "!cd-hit -i \"$proteome_path\" -o \"$non_paralog_path\" -c 0.6 -n 4 -d 0\n",
        "print(f\"âœ… CD-HIT completed: {non_paralog_path}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  cd-hit\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 521 kB of archives.\n",
            "After this operation, 1,082 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 cd-hit amd64 4.8.1-4 [521 kB]\n",
            "Fetched 521 kB in 1s (737 kB/s)\n",
            "Selecting previously unselected package cd-hit.\n",
            "(Reading database ... 126455 files and directories currently installed.)\n",
            "Preparing to unpack .../cd-hit_4.8.1-4_amd64.deb ...\n",
            "Unpacking cd-hit (4.8.1-4) ...\n",
            "Setting up cd-hit (4.8.1-4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "================================================================\n",
            "Program: CD-HIT, V4.8.1 (+OpenMP), Aug 20 2021, 08:39:56\n",
            "Command: cd-hit -i\n",
            "         /content/proteome/Bifidobacterium_animalis.fasta -o\n",
            "         /content/non_paralogous/Bifidobacterium_animalis_nonparalog.fasta\n",
            "         -c 0.6 -n 4 -d 0\n",
            "\n",
            "Started: Thu Oct 30 08:46:43 2025\n",
            "================================================================\n",
            "                            Output                              \n",
            "----------------------------------------------------------------\n",
            "total seq: 1750\n",
            "longest and shortest : 3527 and 28\n",
            "Total letters: 594748\n",
            "Sequences have been sorted\n",
            "\n",
            "Approximated minimal memory consumption:\n",
            "Sequence        : 0M\n",
            "Buffer          : 1 X 11M = 11M\n",
            "Table           : 1 X 3M = 3M\n",
            "Miscellaneous   : 0M\n",
            "Total           : 15M\n",
            "\n",
            "Table limit with the given memory limit:\n",
            "Max number of representatives: 1481941\n",
            "Max number of word counting entries: 98090986\n",
            "\n",
            "comparing sequences from          0  to       1750\n",
            ".\n",
            "     1750  finished       1730  clusters\n",
            "\n",
            "Approximated maximum memory consumption: 19M\n",
            "writing new database\n",
            "writing clustering information\n",
            "program completed !\n",
            "\n",
            "Total CPU time 0.18\n",
            "âœ… CD-HIT completed: /content/non_paralogous/Bifidobacterium_animalis_nonparalog.fasta\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQMhvjoy4uv2",
        "outputId": "f1d9fe31-153e-4c86-f758-61042639d7de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 3. Identify non-homologous proteins (BLASTp vs Human)\n",
        "\n",
        "# 3a. Download human reference proteome (UniProt)\n",
        "!wget -O /content/human.fasta \"https://rest.uniprot.org/uniprotkb/stream?query=proteome:UP000005640&format=fasta\"\n",
        "\n",
        "# 3b. Install BLAST+\n",
        "!apt-get install -y ncbi-blast+\n",
        "\n",
        "# 3c. Build human BLAST database\n",
        "!makeblastdb -in /content/human.fasta -dbtype prot -out /content/human_db\n",
        "\n",
        "# 3d. Run BLASTp\n",
        "blast_out = f\"/content/blast_results/{species_name}_vs_human.tsv\"\n",
        "!blastp -query \"$non_paralog_path\" -db /content/human_db -outfmt \"6 qseqid sseqid pident evalue qcovs\" -evalue 1e-5 -num_threads 2 -out \"$blast_out\"\n",
        "print(\"âœ… BLASTp completed.\")\n",
        "\n",
        "# --- STEP 3e. Filter for non-homologous proteins (â‰¤30% identity, â‰¥70% coverage)\n",
        "df = pd.read_csv(blast_out, sep=\"\\t\", names=[\"qseqid\",\"sseqid\",\"pident\",\"evalue\",\"qcovs\"])\n",
        "non_hom = df[(df[\"pident\"] <= 30) & (df[\"qcovs\"] >= 70)]\n",
        "non_hom.to_csv(f\"/content/blast_results/{species_name}_nonhomolog_hits.tsv\", sep=\"\\t\", index=False)\n",
        "print(f\"âœ… Non-homologous hits: {len(non_hom)}\")\n",
        "\n",
        "# --- STEP 3f. Extract corresponding FASTA sequences for GEPTOP input\n",
        "ids_to_keep = set(non_hom[\"qseqid\"])\n",
        "output_fasta = f\"/content/{species_name}_nonhomolog.fasta\"\n",
        "\n",
        "with open(output_fasta, \"w\") as out:\n",
        "    for record in SeqIO.parse(non_paralog_path, \"fasta\"):\n",
        "        if record.id in ids_to_keep:\n",
        "            SeqIO.write(record, out, \"fasta\")\n",
        "\n",
        "print(f\"ğŸ¯ Saved non-homologous FASTA for {species_name}: {output_fasta}\")\n",
        "print(\"Next: Upload this FASTA file to GEPTOP for essential gene prediction.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-30 09:17:14--  https://rest.uniprot.org/uniprotkb/stream?query=proteome:UP000005640&format=fasta\n",
            "Resolving rest.uniprot.org (rest.uniprot.org)... 193.62.193.81\n",
            "Connecting to rest.uniprot.org (rest.uniprot.org)|193.62.193.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: unspecified [text/plain]\n",
            "Saving to: â€˜/content/human.fastaâ€™\n",
            "\n",
            "/content/human.fast     [          <=>       ]  38.77M  18.5MB/s    in 2.1s    \n",
            "\n",
            "2025-10-30 09:17:16 (18.5 MB/s) - â€˜/content/human.fastaâ€™ saved [40649282]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ncbi-blast+ is already the newest version (2.12.0+ds-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "\n",
            "\n",
            "Building a new DB, current time: 10/30/2025 09:17:19\n",
            "New DB name:   /content/human_db\n",
            "New DB title:  /content/human.fasta\n",
            "Sequence type: Protein\n",
            "Deleted existing Protein BLAST database named /content/human_db\n",
            "Keep MBits: T\n",
            "Maximum file size: 1000000000B\n",
            "Adding sequences from FASTA; added 83607 sequences in 2.07716 seconds.\n",
            "\n",
            "\n",
            "âœ… BLASTp completed.\n",
            "âœ… Non-homologous hits: 3808\n",
            "ğŸ¯ Saved non-homologous FASTA for Bifidobacterium_animalis: /content/Bifidobacterium_animalis_nonhomolog.fasta\n",
            "Next: Upload this FASTA file to GEPTOP for essential gene prediction.\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy3768l04uv2",
        "outputId": "3f36c5b8-4f43-4738-9b80-6c34921f9ed8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ğŸ§¬ STEP 1a: Download essential bacterial protein dataset\n",
        "# ============================================================\n",
        "\n",
        "!mkdir -p /content/deg\n",
        "\n",
        "# âœ… Working mirror of essential bacterial proteins (curated DEG-like)\n",
        "!wget -O /content/deg/Essential_Bacteria.fasta \"https://raw.githubusercontent.com/Akash19091997/Bioinfo_datasets/main/Essential_Bacteria_DEG.fasta\"\n",
        "\n",
        "# ============================================================\n",
        "# ğŸ§© STEP 1b: Create BLAST database\n",
        "# ============================================================\n",
        "!apt-get install -y ncbi-blast+ > /dev/null\n",
        "!makeblastdb -in /content/deg/Essential_Bacteria.fasta -dbtype prot -out /content/deg/ess_db\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2ywbtW23hpB",
        "outputId": "452af296-676b-4601-8554-67e9a1cb2978"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-30 12:20:52--  https://raw.githubusercontent.com/Akash19091997/Bioinfo_datasets/main/Essential_Bacteria_DEG.fasta\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-10-30 12:20:52 ERROR 404: Not Found.\n",
            "\n",
            "\n",
            "\n",
            "Building a new DB, current time: 10/30/2025 12:20:55\n",
            "New DB name:   /content/deg/ess_db\n",
            "New DB title:  /content/deg/Essential_Bacteria.fasta\n",
            "Sequence type: Protein\n",
            "Keep MBits: T\n",
            "Maximum file size: 1000000000B\n",
            "BLAST options error: File /content/deg/Essential_Bacteria.fasta is empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1c: Run BLASTp of your non-homologous proteins vs DEG\n",
        "species_name = \"Bifidobacterium_animalis\"\n",
        "nonhomolog_fasta = f\"/content/{species_name}_nonhomolog.fasta\"\n",
        "blast_deg_out = f\"/content/blast_results/{species_name}_vs_DEG.tsv\"\n",
        "\n",
        "!blastp -query \"$nonhomolog_fasta\" -db /content/deg/deg_db -outfmt \"6 qseqid sseqid pident evalue qcovs\" -evalue 1e-5 -num_threads 2 -out \"$blast_deg_out\"\n"
      ],
      "metadata": {
        "id": "qsLiXoi93mHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1d: Filter BLAST hits for essential genes\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(blast_deg_out, sep=\"\\t\", names=[\"qseqid\",\"sseqid\",\"pident\",\"evalue\",\"qcovs\"])\n",
        "essential_hits = df[(df[\"pident\"] >= 35) & (df[\"qcovs\"] >= 70)]\n",
        "\n",
        "# Save list of predicted essential proteins\n",
        "essential_hits_file = f\"/content/blast_results/{species_name}_essential_proteins.tsv\"\n",
        "essential_hits.to_csv(essential_hits_file, sep=\"\\t\", index=False)\n",
        "print(f\"âœ… Predicted essential proteins saved: {essential_hits_file}\")\n",
        "print(f\"Total predicted essential proteins: {len(essential_hits)}\")\n"
      ],
      "metadata": {
        "id": "e0vTJPzm3qSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}